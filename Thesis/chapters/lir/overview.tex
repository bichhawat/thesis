\section{Quantifying Information Leaks}
\label{sec:bgqif}

The information released, or the \emph{mutual information}, is
generally quantified as the difference in the entropy, which is a
measure of the adversary's uncertainty, of the sensitive information 
before and after the information is released, i.e.,  
$$\text{mutual information = initial uncertainty - final
  uncertainty}$$ 
Roughly speaking, this amounts to the gain in knowledge of the
adversary about a sensitive value. The adversary computes a probable
initial set of values for the sensitive value. By observing the
information released, the adversary can refine his/her knowledge set
by removing the improbable values.
Various information-theoretic measures have been proposed~\cite{shannon, guessing, 
  smith2009, clarkson2009, csf12GLeakage} for quantifying information
leaks. Many of these measures determine an average worst-case bound on 
the amount of information that a program can leak. 

Clark et al.~\cite{clark} propose an approach to statically 
quantify the amount of information leaked in an imperative language.
An important result that they prove in their work is that the information 
released by a program in a deterministic system is equal to the Shannon 
entropy of the outputs given the public inputs. Thus, for computing the 
information released by a program about a secret, it is enough to compute
the Shannon entropy of the public outputs given the public inputs. 
Another important result is the Shannon's source coding 
theorem~\cite{shannon} that intuitively states that 
if one can associate variable-sized bit-codes with different outputs 
of a program such that the codes are uniquely-decodable, 
then the average of the code-lengths of these codes has been shown 
to be bounded by the Shannon entropy of the outputs of the program. 
The soundness of our approach builds on top of these two results. 
We associate bit-codes with the outputs of the program being analyzed 
and show that these codes are uniquely-decodable. In other words, 
the information release as computed by our approach averaged over 
all executions is bounded by the actual information released by the program.
